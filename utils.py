import os
import traceback
from typing import List

# LangChain core modules
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_core.runnables import RunnableSequence

# OpenAI / LLM
from langchain_openai import ChatOpenAI

# ãã®ä»–å…±é€š
import constants as ct


# ==============================================================
# ContextualCompressionRetrieverï¼ˆå‰Šé™¤ã•ã‚ŒãŸæ©Ÿèƒ½ã®ä»£æ›¿å®Ÿè£…ï¼‰
# ==============================================================
class ContextualCompressionRetriever(BaseRetriever):
    """LangChain 0.3+ äº’æ›: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ä»£æ›¿"""

    def __init__(self, base_retriever: BaseRetriever):
        self.base_retriever = base_retriever

    def get_relevant_documents(self, query: str) -> List[Document]:
        """åŒæœŸã§é–¢é€£æ–‡æ›¸ã‚’å–å¾—"""
        docs = self.base_retriever.get_relevant_documents(query)
        # åœ§ç¸®å‡¦ç†ï¼ˆä»Šå›ã¯ç°¡ç•¥åŒ–ï¼‰
        return docs

    async def aget_relevant_documents(self, query: str) -> List[Document]:
        """éåŒæœŸã§é–¢é€£æ–‡æ›¸ã‚’å–å¾—"""
        docs = await self.base_retriever.aget_relevant_documents(query)
        return docs


# ==============================================================
# ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
# ==============================================================
def init_chat_model(model_name: str = "gpt-4o-mini") -> ChatOpenAI:
    """OpenAIãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
    try:
        return ChatOpenAI(
            model=model_name,
            temperature=0.3,
            streaming=False
        )
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
        traceback.print_exc()
        raise


# ==============================================================
# Streamlit ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ç”¨
# ==============================================================
def build_error_message(message: str) -> str:
    """Streamlit ç”¨ã®çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    return f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {message}"


# ==============================================================
# æ¤œç´¢ãƒ»å›ç­”ç”Ÿæˆç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
# ==============================================================
def create_basic_retriever_chain(retriever: VectorStoreRetriever, llm: ChatOpenAI) -> RunnableSequence:
    """Retriever + LLM ã®åŸºæœ¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰"""
    try:
        chain = RunnableSequence(
            steps=[
                ("retriever", retriever),
                ("llm", llm)
            ]
        )
        return chain
    except Exception as e:
        print(f"âŒ RetrieverChain åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        raise


# ==============================================================
# ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
# ==============================================================
def log_debug_info():
    """ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¸»è¦æƒ…å ±ã‚’å‡ºåŠ›"""
    print("ğŸ”§ utils.py loaded successfully")
    print(f"ğŸ”§ OpenAI API Key: {'set' if os.getenv('OPENAI_API_KEY') else 'not set'}")
# ============================================================
# Chatå¿œç­”ç”Ÿæˆç”¨ã®é–¢æ•°
# ============================================================
def generate_answer(prompt: str, mode: str):
    """
    å…¥åŠ›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã€‚
    ç¤¾å†…æ–‡æ›¸æ¤œç´¢ or ç¤¾å†…å•ã„åˆã‚ã›ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã¦å‡¦ç†ã€‚
    """
    from langchain_openai import ChatOpenAI
    from langchain_core.documents import Document
    from langchain_core.prompts import ChatPromptTemplate

    try:
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.4)

        if mode == "ç¤¾å†…æ–‡æ›¸æ¤œç´¢":
            system_prompt = (
                "ã‚ãªãŸã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                "å…¥åŠ›ã«é–¢é€£ã™ã‚‹ç¤¾å†…æ–‡æ›¸ã®å†…å®¹ã‚’è¦ç´„ã—ã€æ­£ç¢ºã‹ã¤ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
            )
        else:
            system_prompt = (
                "ã‚ãªãŸã¯ç¤¾å†…å•ã„åˆã‚ã›å¯¾å¿œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                "è³ªå•ã®èƒŒæ™¯ã‚’è€ƒæ…®ã—ã€åˆ©ç”¨è€…ãŒçŸ¥ã‚ŠãŸã„æƒ…å ±ã‚’æ–‡è„ˆã‹ã‚‰æ¨æ¸¬ã—ã¦ç­”ãˆã¦ãã ã•ã„ã€‚"
            )

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("user", "{input}")
        ])

        chain = prompt_template | llm
        result = chain.invoke({"input": prompt})
        return result.content

    except Exception as e:
        print(f"âŒ generate_answer() failed: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
