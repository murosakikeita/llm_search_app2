import os
import traceback
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
import constants as ct
import utils

# ==============================================================
# åˆæœŸåŒ–å‡¦ç†
# ==============================================================
def initialize():
    """ã‚¢ãƒ—ãƒªå…¨ä½“ã®åˆæœŸåŒ–å‡¦ç†"""
    print("âœ… initialize() started")

    try:
        # -----------------------------
        # 1. ç’°å¢ƒå¤‰æ•°ãƒ­ãƒ¼ãƒ‰
        # -----------------------------
        load_dotenv()
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

        print("ğŸ”‘ OpenAI APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¾ã—ãŸ")

        # -----------------------------
        # 2. ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        # -----------------------------
        persist_directory = os.path.join("logs", "chroma_db")
        os.makedirs(persist_directory, exist_ok=True)
        print(f"ğŸ“‚ ChromaDBç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {persist_directory}")

        # -----------------------------
        # 3. Embedding ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        # -----------------------------
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        print("ğŸ§  Embeddingsãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

        # -----------------------------
        # 4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
        # -----------------------------
        docs_dir = os.path.join("logs", "docs")
        if not os.path.exists(docs_dir):
            os.makedirs(docs_dir, exist_ok=True)

        # å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’ãƒ­ãƒ¼ãƒ‰
        loaders = []
        if os.listdir(docs_dir):
            loaders.append(DirectoryLoader(docs_dir, glob="*.txt", loader_cls=TextLoader))
            loaders.append(DirectoryLoader(docs_dir, glob="*.pdf", loader_cls=PyPDFLoader))
            loaders.append(DirectoryLoader(docs_dir, glob="*.docx", loader_cls=Docx2txtLoader))

            documents = []
            for loader in loaders:
                try:
                    documents.extend(loader.load())
                except Exception as e:
                    print(f"âš ï¸ ä¸€éƒ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

            print(f"ğŸ“„ {len(documents)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        else:
            documents = []
            print("âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ãŒç©ºã§ã™ã€‚ç©ºã®çŠ¶æ…‹ã§ç¶šè¡Œã—ã¾ã™ã€‚")

        # -----------------------------
        # 5. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        # -----------------------------
        if documents:
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            texts = text_splitter.split_documents(documents)
            print(f"ğŸ§© åˆ†å‰²å¾Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(texts)}")
        else:
            texts = []

        # -----------------------------
        # 6. ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆ
        # -----------------------------
        if texts:
            vectordb = Chroma.from_documents(
                documents=texts,
                embedding=embeddings,
                persist_directory=persist_directory
            )
            vectordb.persist()
            print("ğŸ’¾ ChromaDBã¸ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
        else:
            print("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚ChromaDBã®ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

        print("âœ… initialize() completed")

    except Exception as e:
        print(f"âŒ initialize() failed: {e}")
        traceback.print_exc()
        raise e
